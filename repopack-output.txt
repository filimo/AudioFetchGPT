This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-10-17T06:52:05.539Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
AudioFetchGPT/
  Assets.xcassets/
    AccentColor.colorset/
      Contents.json
    AppIcon.appiconset/
      Contents.json
    ArtworkImage.imageset/
      Contents.json
    Contents.json
  AudioModels/
    DownloadedAudio.swift
    DownloadedAudioStore.swift
  AudioPlayback/
    LockScreenHandler.swift
    PlaybackDelegate.swift
    PlaybackManager.swift
    PlayerManager.swift
    ProgressManager.swift
    TimeFormatter.swift
    TimerManager.swift
  ConversationWebView/
    ConversationNavigationDelegate.swift
    conversationScript.js
    ConversationWebView.swift
    ConversationWebViewModel.swift
    ScriptMessageHandler.swift
  NotificationHelpers/
    NotificationExtensions.swift
  Preview Content/
    Preview Assets.xcassets/
      Contents.json
  Views/
    DownloadedAudios/
      AudioRow/
        AudioDetailsView.swift
        AudioRowView.swift
        GotoMessageButton.swift
        PlayPauseButton.swift
        SeekButtonsView.swift
      AudioListView.swift
      DownloadedAudiosListView.swift
      EditConversationView.swift
      SectionHeaderView.swift
    MainContent/
      ControlButtonsView.swift
      MainContentView.swift
      NotificationBannerView.swift
      SearchBarView.swift
  AudioFetchGPTApp.swift
  Info.plist
AudioFetchGPT.xcodeproj/
  project.xcworkspace/
    contents.xcworkspacedata
  xcshareddata/
    xcschemes/
      AudioFetchGPT_Debugger.xcscheme
      AudioFetchGPT.xcscheme
      AudioFetchGPTTests.xcscheme
  project.pbxproj
AudioFetchGPTTests/
  AudioFetchGPTTests.swift
.cursorrules
.gitignore
README.md

================================================================
Repository Files
================================================================

================
File: AudioFetchGPT/Assets.xcassets/AccentColor.colorset/Contents.json
================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: AudioFetchGPT/Assets.xcassets/AppIcon.appiconset/Contents.json
================
{
  "images" : [
    {
      "filename" : "Subject_resized_1024x1024.png",
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "dark"
        }
      ],
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "tinted"
        }
      ],
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: AudioFetchGPT/Assets.xcassets/ArtworkImage.imageset/Contents.json
================
{
  "images" : [
    {
      "filename" : "Subject_resized_1024x1024.png",
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: AudioFetchGPT/Assets.xcassets/Contents.json
================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: AudioFetchGPT/AudioModels/DownloadedAudio.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 16.09.24.
//
import Foundation

struct DownloadedAudio: Identifiable, Codable, Equatable { // Add Equatable conformance
    var id = UUID()
    var relativePath: String
    var fileName: String
    var duration: TimeInterval? // Audio duration in seconds
    var downloadDate: Date // Download time
    let conversationId: String
    var conversationName: String? // Conversation name made optional
    let messageId: String

    enum CodingKeys: String, CodingKey {
        case id
        case relativePath
        case fileName
        case duration
        case downloadDate
        case conversationId
        case conversationName
        case messageId
    }

    init(id: UUID = UUID(), url: URL, fileName: String, duration: TimeInterval?, downloadDate: Date = Date(), conversationId: String, conversationName: String? = nil, messageId: String) {
        self.id = id
        self.relativePath = url.lastPathComponent
        self.fileName = fileName
        self.duration = duration
        self.downloadDate = downloadDate
        self.conversationId = conversationId
        self.conversationName = conversationName ?? conversationId // Default to conversationId if nil
        self.messageId = messageId
    }

    var fileURL: URL {
        let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        return documentsPath.appendingPathComponent(relativePath)
    }
}

================
File: AudioFetchGPT/AudioModels/DownloadedAudioStore.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 16.09.24.
//

import SwiftUI

class DownloadedAudioStore: ObservableObject {
    @Published var items: [DownloadedAudio] = []
    @AppStorage(UserDefaultsKeys.savedMetaData) private var savedMetaData: String = "{}"
    @AppStorage("collapsedSections") private var collapsedSectionIDs: String = "[]"

    private enum UserDefaultsKeys {
        static let downloadedAudios = "downloadedAudios"
        static let savedMetaData = "savedMetaData"
    }
    
    // MARK: - Initialization
    
    init() {
        loadDownloadedAudios()
    }
    
    // MARK: - Public Methods
    
    func loadDownloadedAudios() {
        guard let data = UserDefaults.standard.data(forKey: UserDefaultsKeys.downloadedAudios) else {
            print("No data found for downloaded audios.")
            return
        }
        do {
            let savedAudios = try JSONDecoder().decode([DownloadedAudio].self, from: data)
            items = filterExistingAudios(from: savedAudios)
        } catch {
            print("Error decoding downloaded audios: \(error)")
        }
    }
    
    func addAudio(filePath: URL, fileName: String, duration: TimeInterval?, conversationId: String, messageId: String) {
        let newAudio = DownloadedAudio(url: filePath, fileName: fileName, duration: duration, conversationId: conversationId, messageId: messageId)
        items.append(newAudio)
        saveDownloadedAudios()
        notifyDownloadCompleted(fileName: fileName)
    }
    
    func deleteAudio(_ audio: DownloadedAudio) {
        do {
            try FileManager.default.removeItem(at: audio.fileURL)
            if let index = items.firstIndex(where: { $0.id == audio.id }) {
                items.remove(at: index)
                saveDownloadedAudios()
            }
        } catch {
            print("Failed to delete audio: \(error)")
        }
    }
    
    func updateFileName(for uuid: UUID, name: String) {
        if let index = items.firstIndex(where: { $0.id == uuid }) {
            items[index].fileName = name
            saveDownloadedAudios()
        }
    }
    
    func updateConversationName(conversationId: UUID, newName: String) {
        for index in items.indices {
            if UUID(uuidString: items[index].conversationId) == conversationId {
                items[index].conversationName = newName
            }
        }
        saveDownloadedAudios()
    }

    func getConversationName(by conversationId: UUID) -> String {
        guard let name = items.first(where: { UUID(uuidString: $0.conversationId) == conversationId })?.conversationName else {
            return conversationId.uuidString
        }

        return name.isEmpty ? conversationId.uuidString : name
    }

    func getConversationId(byName name: String) -> String {
        return items.first { $0.conversationName == name }?.conversationId ?? "None ID"
    }
    
    /// Method to move audio files within a specific conversation
    func moveAudio(conversationId: UUID, indices: IndexSet, newOffset: Int) {
        // Filter audio files belonging to the specified conversation
        let conversationAudios = items.enumerated().filter { UUID(uuidString: $0.element.conversationId) == conversationId }
        
        // Extract the indices of the audio files to be moved relative to the conversation
        let audioIndices = indices.compactMap { index in
            items.firstIndex(where: { $0.id == conversationAudios[index].element.id })
        }
        
        // Get the audio files to be moved
        let movedAudios = audioIndices.map { items[$0] }
        
        // Remove the audio files from the main collection using a single change set to minimize UI updates
        items.removeAll { movedAudios.contains($0) }
        
        // Calculate the new position to insert the moved audio files
        let updatedConversationAudios = items.enumerated().filter { UUID(uuidString: $0.element.conversationId) == conversationId }
        let boundedOffset = max(0, min(newOffset, updatedConversationAudios.count))
        
        // Determine the correct index for insertion in the main collection
        let insertionIndex: Int
        if boundedOffset < updatedConversationAudios.count {
            insertionIndex = updatedConversationAudios[boundedOffset].offset
        } else {
            insertionIndex = items.endIndex
        }
        
        // Insert the moved audio files back into the collection at the calculated index
        items.insert(contentsOf: movedAudios, at: insertionIndex)
        
        // Save the changes
        saveDownloadedAudios()
    }

    var collapsedSections: Set<UUID> {
        get {
            let data = Data(collapsedSectionIDs.utf8)
            if let ids = try? JSONDecoder().decode([String].self, from: data) {
                return Set(ids.compactMap { UUID(uuidString: $0) })
            }
            return []
        }
        set {
            if let data = try? JSONEncoder().encode(newValue.map { $0.uuidString }),
               let jsonString = String(data: data, encoding: .utf8)
            {
                collapsedSectionIDs = jsonString
            }
        }
    }
    
    func toggleSection(_ conversationId: UUID) {
        var sections = collapsedSections
        if sections.contains(conversationId) {
            sections.remove(conversationId)
        } else {
            sections.insert(conversationId)
        }
        collapsedSections = sections
    }

    // MARK: - Private Methods
    
    private func saveDownloadedAudios() {
        let existingAudios = filterExistingAudios(from: items)
        if let data = try? JSONEncoder().encode(existingAudios) {
            UserDefaults.standard.set(data, forKey: UserDefaultsKeys.downloadedAudios)
            items = existingAudios
        }
    }
    
    private func filterExistingAudios(from audios: [DownloadedAudio]) -> [DownloadedAudio] {
        audios.filter { FileManager.default.fileExists(atPath: $0.fileURL.path) }
    }
    
    private func notifyDownloadCompleted(fileName: String) {
        NotificationCenter.default.post(name: .audioDownloadCompleted, object: fileName)
    }
}

================
File: AudioFetchGPT/AudioPlayback/LockScreenHandler.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 8.10.24.
//

import AVFoundation
import MediaPlayer
import UIKit

class LockScreenHandler {
    private var audioManager: PlaybackManager?

    init(audioManager: PlaybackManager) {
        self.audioManager = audioManager
        setupRemoteCommandCenter()
    }

    // MARK: - Remote Command Center Setup
    private func setupRemoteCommandCenter() {
        let commandCenter = MPRemoteCommandCenter.shared()
        
        commandCenter.playCommand.addTarget { [weak self] _ in
            guard let self = self, let currentAudio = self.audioManager?.currentAudio else { return .commandFailed }
            self.audioManager?.playAudio(for: currentAudio)
            return .success
        }
        
        commandCenter.pauseCommand.addTarget { [weak self] _ in
            guard let self = self else { return .commandFailed }
            self.audioManager?.pauseAudio()
            return .success
        }
        
        commandCenter.togglePlayPauseCommand.addTarget { [weak self] _ in
            guard let self = self else { return .commandFailed }
            if self.audioManager?.isPlaying == true {
                self.audioManager?.pauseAudio()
            } else if let currentAudio = self.audioManager?.currentAudio {
                self.audioManager?.playAudio(for: currentAudio)
            }
            return .success
        }
        
        commandCenter.changePlaybackPositionCommand.addTarget { [weak self] event in
            guard let self = self, let positionEvent = event as? MPChangePlaybackPositionCommandEvent else {
                return .commandFailed
            }
            self.audioManager?.seek(to: positionEvent.positionTime)
            return .success
        }
        
        commandCenter.nextTrackCommand.addTarget { [weak self] _ in
            guard let self = self else { return .commandFailed }
            self.audioManager?.playNextAudio()
            return .success
        }
        
        commandCenter.previousTrackCommand.addTarget { [weak self] _ in
            guard let self = self else { return .commandFailed }
            self.audioManager?.playPreviousAudio()
            return .success
        }
    }

    // MARK: - Now Playing Info Setup
    func setupNowPlaying(audio: DownloadedAudio, currentTime: Double, isPlaying: Bool, duration: Double) {
        var nowPlayingInfo: [String: Any] = [
            MPMediaItemPropertyTitle: audio.fileName,
            MPNowPlayingInfoPropertyElapsedPlaybackTime: currentTime,
            MPMediaItemPropertyPlaybackDuration: duration,
            MPNowPlayingInfoPropertyPlaybackRate: isPlaying ? 1.0 : 0.0
        ]
        
        if let artworkImage = UIImage(named: "ArtworkImage") {
            let artwork = MPMediaItemArtwork(boundsSize: artworkImage.size) { _ in
                artworkImage
            }
            nowPlayingInfo[MPMediaItemPropertyArtwork] = artwork
        }
        
        MPNowPlayingInfoCenter.default().nowPlayingInfo = nowPlayingInfo
    }

    // MARK: - Update Now Playing Progress
    func updateNowPlayingProgress(currentTime: Double, isPlaying: Bool, duration: Double, queueIndex: Int?, queueCount: Int?) {
        guard MPNowPlayingInfoCenter.default().nowPlayingInfo != nil else { return }
        
        var nowPlayingInfo = MPNowPlayingInfoCenter.default().nowPlayingInfo ?? [:]
        nowPlayingInfo[MPNowPlayingInfoPropertyElapsedPlaybackTime] = currentTime
        nowPlayingInfo[MPNowPlayingInfoPropertyPlaybackRate] = isPlaying ? 1.0 : 0.0
        nowPlayingInfo[MPMediaItemPropertyPlaybackDuration] = duration
        if let queueIndex = queueIndex {
            nowPlayingInfo[MPNowPlayingInfoPropertyPlaybackQueueIndex] = queueIndex
        }
        if let queueCount = queueCount {
            nowPlayingInfo[MPNowPlayingInfoPropertyPlaybackQueueCount] = queueCount
        }
        
        MPNowPlayingInfoCenter.default().nowPlayingInfo = nowPlayingInfo
    }
}

================
File: AudioFetchGPT/AudioPlayback/PlaybackDelegate.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 18.09.24.
//


import AVFoundation

class PlaybackDelegate: NSObject, AVAudioPlayerDelegate {
    weak var audioManager: PlaybackManager?
    
    init(audioManager: PlaybackManager) {
        self.audioManager = audioManager
        super.init()
    }
    
    func audioPlayerDidFinishPlaying(_ player: AVAudioPlayer, successfully flag: Bool) {
        guard flag else { return }
        DispatchQueue.main.async { [weak self] in
            self?.audioManager?.handleAudioFinished()
        }
    }
}

================
File: AudioFetchGPT/AudioPlayback/PlaybackManager.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 17.09.24.
//

import AVFoundation
import Combine
import MediaPlayer
import NotificationCenter
import SwiftUI

class PlaybackManager: ObservableObject {
    @Published var isPlaying = false
    @AppStorage("currentAudioID") var currentAudioIDString: String = ""
    @Published private(set) var currentTime: Double = 0
    @Published var messageId: String = ""
    
    @AppStorage("playbackRate") private var playbackRateStorage: Double = 1.0
    var playbackRate: Float {
        get { Float(playbackRateStorage) }
        set { playbackRateStorage = Double(newValue) }
    }

    private let playerManager = PlayerManager()
    private let progressManager = ProgressManager()
    private var timerManager = TimerManager()
    private var currentAudioPrivate: DownloadedAudio?
    private var audioPlayerDelegate: PlaybackDelegate?
    
    private var downloadedAudios: DownloadedAudioStore
    private var lockScreenManager: LockScreenHandler?

    init(downloadedAudios: DownloadedAudioStore) {
        self.downloadedAudios = downloadedAudios
        audioPlayerDelegate = PlaybackDelegate(audioManager: self)
        playerManager.delegate = audioPlayerDelegate
        
        timerManager.updateAction = { [weak self] in
            self?.updateProgress()
        }
        
        setupLockScreenManager()
        
        NotificationCenter.default.addObserver(forName: .AVPlayerItemDidPlayToEndTime, object: nil, queue: .main) { [weak self] _ in
            self?.handleAudioFinished()
        }
    }
    
    private func setupLockScreenManager() {
        lockScreenManager = LockScreenHandler(audioManager: self)
    }
    
    var currentProgress: Double {
        get { progressManager.getProgress(for: currentAudioID) }
        set { progressManager.setProgress(newValue, for: currentAudioID) }
    }

    var currentAudioID: UUID {
        get { UUID(uuidString: currentAudioIDString) ?? UUID() }
        set { currentAudioIDString = newValue.uuidString }
    }
    
    // Публичное свойство для доступа к currentAudio
    var currentAudio: DownloadedAudio? {
        return currentAudioPrivate
    }
    
    func playAudio(for audio: DownloadedAudio) {
        if currentAudioPrivate?.id != audio.id {
            prepareNewAudio(audio)
        }

        togglePlayPause(for: audio)
    }
    
    private func prepareNewAudio(_ audio: DownloadedAudio) {
        guard playerManager.preparePlayer(for: audio.fileURL) else { return }
        currentAudioID = audio.id
        currentAudioPrivate = audio
        seekAudio(for: audio, to: currentProgress)
        setPlaybackRate(Float(playbackRate))
    }
    
    private func togglePlayPause(for audio: DownloadedAudio) {
        if isPlaying && currentAudioPrivate?.id == audio.id {
            pauseAudio()
        } else {
            startPlayback(for: audio)
        }
    }
    
    private func startPlayback(for audio: DownloadedAudio) {
        timerManager.startTimer()
        playerManager.play()
        isPlaying = true
        updateNowPlayingProgress()
        setupNowPlaying(audio: audio)
    }
    
    func pauseAudio() {
        playerManager.pause()
        isPlaying = false
        timerManager.stopTimer()
        updateNowPlayingProgress()
    }
    
    func seekAudio(for audio: DownloadedAudio, to progress: Double) {
        let newTime = progress * playerManager.duration
        playerManager.seek(to: newTime)
        currentTime = newTime
        progressManager.setCurrentTime(newTime, for: audio.id)
        updateNowPlayingProgress()
    }
    
    func seekBySeconds(for audio: DownloadedAudio, seconds: Double) {
        let newTime = max(0, min(playerManager.currentTime + seconds, playerManager.duration))
        playerManager.seek(to: newTime)
        currentProgress = newTime / playerManager.duration
        currentTime = newTime
        updateNowPlayingProgress()
    }
    
    private func updateProgress() {
        let progress = playerManager.currentTime / playerManager.duration
        currentTime = playerManager.currentTime
        progressManager.setProgress(progress, for: currentAudioID)
        progressManager.setCurrentTime(playerManager.currentTime, for: currentAudioID)
        
        if isPlaying {
            updateNowPlayingProgress()
        }
    }
    
    func handleAudioFinished() {
        pauseAudio()
        playNextAudio()
    }
    
    func currentTimeForAudio(_ audioID: UUID) -> String {
        let time = audioID == currentAudioID ? currentTime : progressManager.getCurrentTime(for: audioID)
        return TimeFormatter.formatTime(time)
    }
    
    func progressForAudio(_ audioID: UUID) -> Double {
        return progressManager.getProgress(for: audioID)
    }
    
    func seekAudio(for audioID: UUID, to progress: Double) {
        let newTime = progress * playerManager.duration
        if currentAudioPrivate?.id == audioID {
            playerManager.seek(to: newTime)
            currentTime = newTime
        }
        progressManager.setCurrentTime(newTime, for: audioID)
        progressManager.setProgress(progress, for: audioID)
        updateNowPlayingProgress()
    }
    
    // MARK: - Now Playing Info
    
    func setupNowPlaying(audio: DownloadedAudio) {
        lockScreenManager?.setupNowPlaying(audio: audio, currentTime: currentTime, isPlaying: isPlaying, duration: playerManager.duration)
    }
    
    private func updateNowPlayingProgress() {
        guard let currentAudio = currentAudio else { return }
        
        let queueIndex = downloadedAudios.items.firstIndex(where: { $0.id == currentAudio.id })
        let queueCount = downloadedAudios.items.count
        
        lockScreenManager?.updateNowPlayingProgress(currentTime: currentTime, isPlaying: isPlaying, duration: playerManager.duration, queueIndex: queueIndex, queueCount: queueCount)
    }
    
    // MARK: - Playback Controls
    
    func playNextAudio() {
        guard let currentAudio = currentAudioPrivate,
              let currentIndex = downloadedAudios.items.firstIndex(where: { $0.id == currentAudio.id }),
              currentIndex + 1 < downloadedAudios.items.count
        else {
            // Нет следующего аудио для воспроизведения
            return
        }
        
        let nextAudio = downloadedAudios.items[currentIndex + 1]
        playAudio(for: nextAudio)
    }
    
    func playPreviousAudio() {
        guard let currentAudio = currentAudioPrivate,
              let currentIndex = downloadedAudios.items.firstIndex(where: { $0.id == currentAudio.id }),
              currentIndex - 1 >= 0
        else {
            // Нет предыдущего аудио для воспроизведения
            return
        }
        
        let previousAudio = downloadedAudios.items[currentIndex - 1]
        playAudio(for: previousAudio)
    }
    
    // MARK: - Добавленный метод для поиска позиции воспроизведения
    
    func seek(to position: Double) {
        guard let currentAudio = currentAudioPrivate else { return }
        // Убедимся, что position не превышает длительность
        let clampedPosition = max(0, min(position, playerManager.duration))
        seekAudio(for: currentAudio, to: clampedPosition / playerManager.duration)
    }
    
    func changePlaybackRate(to rate: Float) {
        playbackRate = rate
        playerManager.setPlaybackRate(rate)
    }
    
    func setPlaybackRate(_ rate: Float) {
        playbackRate = rate
        playerManager.setPlaybackRate(rate)
    }
}

================
File: AudioFetchGPT/AudioPlayback/PlayerManager.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 18.09.24.
//

import AVFoundation

class PlayerManager {
    private var audioPlayer: AVAudioPlayer?
    weak var delegate: PlaybackDelegate?

    var isPlaying: Bool {
        return audioPlayer?.isPlaying ?? false
    }

    @Published var playbackRate: Float = 1.0 // Current playback rate

    func preparePlayer(for url: URL) -> Bool {
        guard FileManager.default.fileExists(atPath: url.path) else {
            print("Audio file not found at path: \(url.path)")
            return false
        }

        do {
            try AVAudioSession.sharedInstance().setCategory(.playback, mode: .default)
            try AVAudioSession.sharedInstance().setActive(true)

            audioPlayer = try AVAudioPlayer(contentsOf: url)
            audioPlayer?.delegate = delegate
            audioPlayer?.enableRate = true // Enable rate adjustment
            return true
        } catch {
            print("Failed to prepare audio player: \(error)")
            return false
        }
    }

    func play() {
        audioPlayer?.play()
    }

    func pause() {
        audioPlayer?.pause()
    }

    func seek(to time: TimeInterval) {
        audioPlayer?.currentTime = time
    }

    var currentTime: TimeInterval {
        audioPlayer?.currentTime ?? 0
    }

    var duration: TimeInterval {
        audioPlayer?.duration ?? 0
    }

    func setPlaybackRate(_ rate: Float) {
        playbackRate = rate
        audioPlayer?.rate = rate
    }
}

================
File: AudioFetchGPT/AudioPlayback/ProgressManager.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 18.09.24.
//

import Foundation
import SwiftUI

class ProgressManager {
    @AppStorage("audioProgress") private var audioProgressData: String = "{}"
    @AppStorage("audioCurrentTime") private var audioCurrentTimeData: String = "{}"

    private var allProgress: [UUID: Double] {
        get {
            if let data = audioProgressData.data(using: .utf8) {
                return (try? JSONDecoder().decode([UUID: Double].self, from: data)) ?? [:]
            }
            return [:]
        }
        set {
            if let data = try? JSONEncoder().encode(newValue),
               let jsonString = String(data: data, encoding: .utf8)
            {
                audioProgressData = jsonString
            }
        }
    }

    private var currentTimes: [UUID: TimeInterval] {
        get {
            if let data = audioCurrentTimeData.data(using: .utf8) {
                return (try? JSONDecoder().decode([UUID: TimeInterval].self, from: data)) ?? [:]
            }
            return [:]
        }
        set {
            if let data = try? JSONEncoder().encode(newValue),
               let jsonString = String(data: data, encoding: .utf8)
            {
                audioCurrentTimeData = jsonString
            }
        }
    }

    func setProgress(_ progress: Double, for audioID: UUID) {
        allProgress[audioID] = progress
    }

    func getProgress(for audioID: UUID) -> Double {
        allProgress[audioID] ?? 0
    }

    func setCurrentTime(_ time: TimeInterval, for audioID: UUID) {
        currentTimes[audioID] = time
    }

    func getCurrentTime(for audioID: UUID) -> TimeInterval {
        currentTimes[audioID] ?? 0
    }

    func resetProgress(for audioID: UUID) {
        allProgress[audioID] = 0
        currentTimes[audioID] = 0
    }
}

================
File: AudioFetchGPT/AudioPlayback/TimeFormatter.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 18.09.24.
//

import Foundation

enum TimeFormatter {
    static func formatTime(_ time: TimeInterval) -> String {
        let formatter = DateComponentsFormatter()
        formatter.allowedUnits = [.minute, .second]
        formatter.zeroFormattingBehavior = .pad
        return formatter.string(from: time) ?? "00:00"
    }

    static func formatDate(_ date: Date) -> String {
        let formatter = DateFormatter()
        formatter.dateStyle = .short
        formatter.timeStyle = .short
        return formatter.string(from: date)
    }
}

================
File: AudioFetchGPT/AudioPlayback/TimerManager.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 18.09.24.
//


import Foundation

class TimerManager {
    private var timer: Timer?
    var updateAction: (() -> Void)?

    func startTimer() {
        stopTimer()
        timer = Timer.scheduledTimer(withTimeInterval: 1, repeats: true) { [weak self] _ in
            self?.updateAction?()
        }
    }

    func stopTimer() {
        timer?.invalidate()
        timer = nil
    }
}

================
File: AudioFetchGPT/ConversationWebView/ConversationNavigationDelegate.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 16.09.24.
//

import WebKit
import SwiftUI

final class ConversationNavigationDelegate: NSObject, WKNavigationDelegate {
    weak var viewModel: ConversationWebViewModel?
    
    init(viewModel: ConversationWebViewModel) {
        self.viewModel = viewModel
    }
    
    func webView(_ webView: WKWebView, didFinish navigation: WKNavigation!) {
        viewModel?.gotoMessage()
        
        // Saving the current URL
        if let currentURL = webView.url?.absoluteString {
            viewModel?.lastVisitedURL = currentURL
        }
    }
}

================
File: AudioFetchGPT/ConversationWebView/conversationScript.js
================
(function() {
    console.log('Start handling /backend-api/synthesize');

    const originalFetch = window.fetch;
    const queue = [];
    let processing = false;

    // Function to process the queue
    function processQueue() {
        if (queue.length === 0 || processing) {
            return;
        }

        processing = true;
        const { input, init, resolve, reject } = queue.shift();

        originalFetch(input, init)
            .then(response => handleResponse(response, input, resolve, reject))
            .catch(error => handleFetchError(error, reject));
    }

    // Function to handle the response
    function handleResponse(response, input, resolve, reject) {
        response.clone().blob()
            .then(blob => processBlob(blob, input, response, resolve, reject))
            .catch(error => handleBlobError(error, reject));
    }

    // Function to process the blob
    function processBlob(blob, input, response, resolve, reject) {
        const url = new URL(input, window.location.origin);
        const conversationId = url.searchParams.get('conversation_id') || '';
        const messageId = url.searchParams.get('message_id') || '';
        const name = document.querySelector(`[data-message-id="${messageId}"]`).innerText;

        const key = `${conversationId}_${messageId}`;
        if (localStorage.getItem(key)) {
            console.log(`Combination ${key} already processed, skipping fetch.`);
            resolve(response);
            processing = false;
            processQueue();
            return;
        }

        const reader = new FileReader();
        reader.onloadend = () => sendMessageToWebKit(reader.result, conversationId, messageId, name, resolve, response);
        reader.readAsDataURL(blob);
    }

    // Function to send the message to WebKit
    function sendMessageToWebKit(audioData, conversationId, messageId, name, resolve, response) {
        window.webkit.messageHandlers.audioHandler.postMessage({
            conversationId,
            messageId,
            audioData,
            name
        });

        const key = `${conversationId}_${messageId}`;
        localStorage.setItem(key, 'processed');

        resolve(response);
        processing = false;
        processQueue();
    }

    // Function to handle blob errors
    function handleBlobError(error, reject) {
        console.error('Error reading blob:', error);
        reject(error);
        processing = false;
        processQueue();
    }

    // Function to handle fetch errors
    function handleFetchError(error, reject) {
        console.error('Fetch error:', error);
        reject(error);
        processing = false;
        processQueue();
    }

    // Override the fetch function
    window.fetch = function(input, init) {
        if (typeof input === 'string' && input.includes('/backend-api/synthesize')) {
            const url = new URL(input, window.location.origin);
            const conversationId = url.searchParams.get('conversation_id') || '';
            const messageId = url.searchParams.get('message_id') || '';

            const key = `${conversationId}_${messageId}`;
            if (localStorage.getItem(key)) {
                console.log(`Combination ${key} already processed, fetch aborted.`);
                return Promise.resolve(new Response(null, { status: 409, statusText: 'Conflict: already processed' }));
            }

            return new Promise((resolve, reject) => {
                queue.push({ input, init, resolve, reject });
                processQueue();
            });
        }
        return originalFetch(input, init);
    };
})();

================
File: AudioFetchGPT/ConversationWebView/ConversationWebView.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 16.09.24.
//

import SwiftUI
import WebKit

struct ConversationWebView: UIViewRepresentable {
    @EnvironmentObject var downloadedAudios: DownloadedAudioStore

    @ObservedObject var viewModel: ConversationWebViewModel

    func makeUIView(context: Context) -> WKWebView {
        viewModel.configureWebView()

        viewModel.webView.configuration.userContentController.add(context.coordinator, name: "audioHandler")

        return viewModel.webView
    }

    func updateUIView(_ uiView: WKWebView, context: Context) {
        // Empty implementation since the search is now handled through WebViewModel
    }

    func makeCoordinator() -> ScriptMessageHandler {
        return ScriptMessageHandler(downloadedAudios: downloadedAudios)
    }
}

================
File: AudioFetchGPT/ConversationWebView/ConversationWebViewModel.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 16.09.24.
//

import SwiftUI
import WebKit

final class ConversationWebViewModel: ObservableObject {
    @Published var webView: WKWebView = .init()
    
    @AppStorage("lastVisitedURL") var lastVisitedURL: String = "https://chatgpt.com"
    
    private var navigationDelegate: ConversationNavigationDelegate?
    private var targetMessageId: String?
    private var currentReadAloudIndex: Int = 0
    
    init() {
        webView.isInspectable = true
        setupNavigationDelegate()
    }
    
    private func setupNavigationDelegate() {
        let delegate = ConversationNavigationDelegate(viewModel: self)
        navigationDelegate = delegate
        webView.navigationDelegate = delegate
    }
    
    func configureWebView() {
        do {
            // Загрузка JavaScript скрипта
            let jsScriptURL = Bundle.main.url(forResource: "conversationScript", withExtension: "js")
            guard let jsScriptURL = jsScriptURL else { throw NSError(domain: "Invalid script URL", code: 0) }
            let jsScript = try String(contentsOf: jsScriptURL, encoding: .utf8)
            
            let userScript = WKUserScript(source: jsScript, injectionTime: .atDocumentStart, forMainFrameOnly: false)
            webView.configuration.userContentController.addUserScript(userScript)
        } catch {
            print("Error loading JavaScript: \(error.localizedDescription)")
            return
        }
        
        let url = URL(string: lastVisitedURL)!
        
        let request = URLRequest(url: url)
        webView.load(request)
        
        // Сохранение текущего URL
        DispatchQueue.main.async {
            self.lastVisitedURL = url.absoluteString
        }
    }
    
    func reload() {
        webView.reload()
    }
    
    func performSearch(text: String, forward: Bool) {
        guard !text.isEmpty else { return }
        
        let script = "window.find('\(text)', false, \(!forward), true)"
        webView.evaluateJavaScript(script) { _, error in
            if let error = error {
                print("Search error: \(error.localizedDescription)")
            }
        }
    }
    
    func gotoMessage(conversationId: String, messageId: String) {
        if let url = URL(string: "https://chatgpt.com/c/\(conversationId)") {
            if webView.url?.absoluteString == url.absoluteString {
                gotoMessage(messageId: messageId)
            } else {
                targetMessageId = messageId
                webView.load(URLRequest(url: url))
            }
        }
    }
    
    func gotoMessage(messageId: String) {
        let script = """
            document.querySelector('[data-message-id="\(messageId)"]').scrollIntoView({
                behavior: 'smooth',
                block: 'start'
            });
        """
        
        webView.evaluateJavaScript(script) { _, error in
            if let error = error {
                print("Navigation error: \(error.localizedDescription)")
            }
        }
    }
    
    func gotoMessage() {
        guard let messageId = targetMessageId else { return }
        
        let script = """
        function waitForElement(selector, callback) {
            const observer = new MutationObserver((mutations, obs) => {
                const element = document.querySelector(selector);
                if (element) {
                    obs.disconnect();
                    callback(element);
                }
            });
            observer.observe(document, { childList: true, subtree: true });
        }
        
        waitForElement('[data-message-id="\(messageId)"]', function(element) {
            window.stop();
            setTimeout(() => {
                element.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }, 3000);
        });
        """
        
        webView.evaluateJavaScript(script) { _, error in
            if let error = error {
                print("Error executing script: \(error.localizedDescription)")
            }
        }
        
        targetMessageId = nil
    }
    
    func sayChatGPT(_ text: String) {
        guard let jsonData = try? JSONEncoder().encode(text),
              let jsonString = String(data: jsonData, encoding: .utf8)
        else {
            print("Text encoding error")
            return
        }
        
        let script = """
            (function() {
                document.querySelector('#prompt-textarea').innerText += \(jsonString);
                setTimeout(() => {
                    document.querySelector('[data-testid="send-button"]').click();
                }, 300);
            })();
        """
        webView.evaluateJavaScript(script) { _, error in
            if let error = error {
                print("Text insertion error: \(error.localizedDescription)")
            }
        }
    }
    
    func scrollToReadAloudElement(at index: Int) {
        let script = """
            (function() {
                var elements = document.querySelectorAll('[data-testid="voice-play-turn-action-button"]');
                if (elements.length > \(index)) {
                    elements[\(index)].scrollIntoView({ behavior: 'smooth', block: 'end' });
                } else {
                    console.error('Index out of bounds: No element at the given index');
                }
            })();            
        """
        
        webView.evaluateJavaScript(script) { _, error in
            if let error = error {
                print("Scroll to element error: \(error.localizedDescription)")
            }
        }
    }
    
    func scrollToNextReadAloudElement() {
        currentReadAloudIndex += 1
        scrollToReadAloudElement(at: currentReadAloudIndex)
    }
    
    func scrollToPreviousReadAloudElement() {
        currentReadAloudIndex = max(0, currentReadAloudIndex - 1)
        scrollToReadAloudElement(at: currentReadAloudIndex)
    }

    func clickAllVoicePlayTurnActionButtons() {
        let script = """
            (function() {
                document.querySelectorAll('[data-testid="voice-play-turn-action-button"]').forEach(el => {
                    el.click();
                });
            })();
        """
        
        webView.evaluateJavaScript(script) { _, error in
            if let error = error {
                print("Click all voice play turn action buttons error: \(error.localizedDescription)")
            }
        }
    }

    @MainActor
    func removeProcessedAudioItem(conversationId: String, messageId: String) async throws {
        let script = """
            (function() {
                localStorage.removeItem('\(conversationId)_\(messageId)');
            })();
        """
        
        try await withCheckedThrowingContinuation { (continuation: CheckedContinuation<Void, Error>) in
            webView.evaluateJavaScript(script) { _, error in
                if let error = error {
                    print("Remove processed audio item error: \(error.localizedDescription)")
                    continuation.resume(throwing: error)
                } else {
                    continuation.resume()
                }
            }
        }
    }
}

================
File: AudioFetchGPT/ConversationWebView/ScriptMessageHandler.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 16.09.24.
//
import AVFAudio
import SwiftUI
import WebKit
import AudioToolbox // Импортируем для воспроизведения системных звуков
import UIKit // Импортируем для использования вибрации

class ScriptMessageHandler: NSObject, WKScriptMessageHandler {
    @ObservedObject var downloadedAudios: DownloadedAudioStore
    
    init(downloadedAudios: DownloadedAudioStore) {
        self.downloadedAudios = downloadedAudios
    }

    func userContentController(_ userContentController: WKUserContentController, didReceive message: WKScriptMessage) {
        if message.name == "audioHandler" {
            if let body = message.body as? [String: Any] {
                if let conversationId = body["conversationId"] as? String,
                   let messageId = body["messageId"] as? String,
                   let audioData = body["audioData"] as? String,
                   let name = body["name"] as? String
                {
                    print("conversationId: \(conversationId), messageId: \(messageId)")
                        
                    downloadAudio(from: audioData, conversationId: conversationId, messageId: messageId, name: name)
                } else {
                    print("Error: Unable to extract values from the object")
                }
            } else {
                print("Error: message.body is not a dictionary")
            }
        }
    }

    private func downloadAudio(from url: String, conversationId: String, messageId: String, name: String) {
        guard let audioURL = URL(string: url) else { return }
        URLSession.shared.dataTask(with: audioURL) { data, _, error in
            guard let data = data, error == nil else { return }
            
            let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
            let fileName = UUID().uuidString
            let filePath = documentsPath.appendingPathComponent(fileName.appending(".m4a"))
            
            do {
                try data.write(to: filePath)
                let audioPlayer = try AVAudioPlayer(contentsOf: filePath)
                audioPlayer.prepareToPlay()
                let duration = audioPlayer.duration
                
                self.downloadedAudios.addAudio(filePath: filePath, fileName: name, duration: duration, conversationId: conversationId, messageId: messageId)
                
                self.playSignalSoundAndVibrate() // Play sound and vibrate after saving the file
            } catch {
                print("Failed to save audio file: \(error)")
            }
        }.resume()
    }
    
    private func playSignalSoundAndVibrate() {
        // Play a system sound (e.g., mail sent sound)
        AudioServicesPlaySystemSound(1013) //"Тон успеха"
        
        // Trigger device vibration
        AudioServicesPlaySystemSound(kSystemSoundID_Vibrate)
    }
}

================
File: AudioFetchGPT/NotificationHelpers/NotificationExtensions.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 20.09.24.
//

import Foundation

extension Notification.Name {
    static let audioDownloadCompleted = Notification.Name("audioDownloadCompleted")
    // You can add other custom notifications here in the future
}

================
File: AudioFetchGPT/Preview Content/Preview Assets.xcassets/Contents.json
================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: AudioFetchGPT/Views/DownloadedAudios/AudioRow/AudioDetailsView.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 17.09.24.
//
import AVFoundation
import SwiftUI

struct AudioDetailsView: View {
    @EnvironmentObject var downloadedAudios: DownloadedAudioStore
    @EnvironmentObject var audioManager: PlaybackManager
    let audio: DownloadedAudio
    @Binding var editableName: String
    @State private var isEditingName: Bool = false

    var body: some View {
        VStack(alignment: .leading) {
            if isEditingName {
                ZStack(alignment: .bottomTrailing) {
                    TextEditor(text: $editableName)
                        .frame(height: 400)
                        .padding()
                        .overlay(
                            RoundedRectangle(cornerRadius: 10)
                                .stroke(Color.gray.opacity(0.5), lineWidth: 1)
                        )
                        .padding()

                    Button(action: {
                        downloadedAudios.updateFileName(for: audio.id, name: editableName)
                        isEditingName = false
                    }) {
                        Text("OK")
                            .foregroundColor(.white)
                            .padding(10)
                            .background(Color.blue)
                            .cornerRadius(8)
                            .shadow(radius: 5)
                    }
                    .padding([.trailing, .bottom], 15)
                }
                .padding(.bottom, 5)

            } else {
                Text(editableName)
                    .font(.headline)
                    .underline(audioManager.currentAudioID == audio.id, color: .yellow)
                    .truncationMode(.tail)
                    .lineLimit(1)
                    .padding(.bottom, 5)
                    .onTapGesture {
                        isEditingName = true
                    }
            }

            HStack {
                Text("\(TimeFormatter.formatDate(audio.downloadDate))")

                if let duration = audio.duration {
                    Text("Duration: \(TimeFormatter.formatTime(duration))")
                }
                Spacer()
            }
            .font(.subheadline)
            .foregroundColor(.gray)
        }
        .onAppear {
            editableName = audio.fileName
        }
    }
}

================
File: AudioFetchGPT/Views/DownloadedAudios/AudioRow/AudioRowView.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 17.09.24.
//
import AVFoundation
import SwiftUI

struct AudioRowView: View {
    @Environment(\.dismiss) var dismiss
    @EnvironmentObject var audioManager: PlaybackManager

    let audio: DownloadedAudio
    @State private var editableName: String = ""

    var body: some View {
        VStack(alignment: .leading) {
            AudioDetailsView(audio: audio, editableName: $editableName)

            Slider(value: Binding(
                get: { audioManager.progressForAudio(audio.id) },
                set: { newValue in
                    audioManager.seekAudio(for: audio.id, to: newValue)
                }
            ), in: 0 ... 1)

            HStack {
                PlayPauseButton(audio: audio)

                GotoMessageButton(conversationId: audio.conversationId, messageId: audio.messageId)

                Spacer()

                SeekButtonsView(audio: audio)

                Spacer()
            }
        }
        .padding(.vertical, 10)
    }
}

================
File: AudioFetchGPT/Views/DownloadedAudios/AudioRow/GotoMessageButton.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 24.09.24.
//

import SwiftUI

struct GotoMessageButton: View {
    let conversationId: String
    let messageId: String
    @EnvironmentObject var audioManager: PlaybackManager
    @EnvironmentObject var webViewModel: ConversationWebViewModel

    var body: some View {
        Button(action: {
            audioManager.messageId = messageId
            webViewModel.gotoMessage(conversationId: conversationId, messageId: messageId)
        }) {
            Image(systemName: "arrowshape.right.fill")
                .resizable()
                .frame(width: 30, height: 30)
                .foregroundStyle(.green)
        }
        .buttonStyle(BorderlessButtonStyle())
    }
}

================
File: AudioFetchGPT/Views/DownloadedAudios/AudioRow/PlayPauseButton.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 17.09.24.
//
import SwiftUI

struct PlayPauseButton: View {
    let audio: DownloadedAudio
    @EnvironmentObject var audioManager: PlaybackManager
    @EnvironmentObject var downloadedAudios: DownloadedAudioStore

    var body: some View {
        Button(action: {
            if audioManager.isPlaying, audioManager.currentAudioID == audio.id {
                audioManager.pauseAudio()
            } else {
                audioManager.playAudio(for: audio)
                audioManager.setupNowPlaying(audio: audio)
            }
        }) {
            Image(systemName: (audioManager.isPlaying && audioManager.currentAudioID == audio.id) ? "pause.circle.fill" : "play.circle.fill")
                .resizable()
                .frame(width: 30, height: 30)
        }
        .buttonStyle(BorderlessButtonStyle())
    }
}

================
File: AudioFetchGPT/Views/DownloadedAudios/AudioRow/SeekButtonsView.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 24.09.24.
//

import SwiftUI

struct SeekButtonsView: View {
    @EnvironmentObject var audioManager: PlaybackManager
    let audio: DownloadedAudio

    var body: some View {
        HStack {
            Button(action: {
                audioManager.seekBySeconds(for: audio, seconds: -5.0)
            }) {
                Image(systemName: "gobackward.5")
                    .resizable()
                    .frame(width: 30, height: 30)
            }
            .buttonStyle(BorderlessButtonStyle())

            Text(audioManager.currentTimeForAudio(audio.id))
                .font(.subheadline)
                .monospacedDigit()
                .frame(minWidth: 50)

            Button(action: {
                audioManager.seekBySeconds(for: audio, seconds: +5.0)
            }) {
                Image(systemName: "goforward.5")
                    .resizable()
                    .frame(width: 30, height: 30)
            }
            .buttonStyle(BorderlessButtonStyle())
        }
    }
}

================
File: AudioFetchGPT/Views/DownloadedAudios/AudioListView.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 6.10.24.
//  Modified to support reordering by [Your Name] on [Date].
//

import SwiftUI

struct AudioListView: View {
    var audios: [DownloadedAudio]
    var onDelete: (DownloadedAudio) -> Void
    var onMove: (IndexSet, Int) -> Void // Callback for moving items

    var body: some View {
        ForEach(audios) { audio in
            AudioRowView(audio: audio)
                .id(audio.id)
                .swipeActions(edge: .trailing, allowsFullSwipe: false) {
                    Button(role: .destructive) {
                        onDelete(audio)
                    } label: {
                        Label("Delete", systemImage: "trash")
                    }
                }
        }
        .onMove(perform: onMove) // Enable moving
    }
}

================
File: AudioFetchGPT/Views/DownloadedAudios/DownloadedAudiosListView.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 16.09.24.
//
import AVFoundation
import SwiftUI

struct DownloadedAudiosListView: View {
    @EnvironmentObject var downloadedAudios: DownloadedAudioStore
    @EnvironmentObject var webViewModel: ConversationWebViewModel
    @EnvironmentObject var audioManager: PlaybackManager
    @EnvironmentObject var viewModel: ConversationWebViewModel

    @State private var editingConversationId: UUID?
    @State private var newConversationName: String = ""

    @State var showErrorAlert: Bool = false
    @State var errorMessage: String = ""

    var groupedAudios: [UUID: [DownloadedAudio]] {
        Dictionary(grouping: downloadedAudios.items, by: { UUID(uuidString: $0.conversationId)! })
    }

    var body: some View {
        NavigationView {
            VStack {
                HStack {
                    Text("Speed: \(audioManager.playbackRate, specifier: "%.1f")x") // Display current playback rate

                    // Кнопка для уменьшения скорости воспроизведения
                    Button(action: {
                        if audioManager.playbackRate > 0.5 {
                            audioManager.setPlaybackRate(audioManager.playbackRate - 0.1)
                        }
                    }) {
                        Image(systemName: "minus.circle")
                            .padding(.trailing, 5)
                    }

                    // Слайдер для регулировки скорости
                    Slider(value: Binding(
                        get: { audioManager.playbackRate },
                        set: { newValue in
                            audioManager.setPlaybackRate(newValue)
                        }
                    ), in: 0.5 ... 2.0, step: 0.1) // Slider for selecting playback rate

                    // Кнопка для увеличения скорости воспроизведения
                    Button(action: {
                        if audioManager.playbackRate < 2.0 {
                            audioManager.setPlaybackRate(audioManager.playbackRate + 0.1)
                        }
                    }) {
                        Image(systemName: "plus.circle")
                            .padding(.leading, 5)
                    }
                }
                .padding(.horizontal, 15)

                ScrollViewReader { reader in
                    List {
                        ForEach(groupedAudios.keys.sorted(), id: \.self) { conversationId in
                            Section(header: SectionHeaderView(conversationId: conversationId,
                                                          conversationName: downloadedAudios.getConversationName(by: conversationId),
                                                          onEdit: { startEditing(conversationId) },
                                                          onToggle: { downloadedAudios.toggleSection(conversationId) },
                                                          isCollapsed: downloadedAudios.collapsedSections.contains(conversationId))
                            ) {
                                if !downloadedAudios.collapsedSections.contains(conversationId) {
                                    AudioListView(audios: groupedAudios[conversationId] ?? [],
                                                  onDelete: deleteAudio,
                                                  onMove: { indices, newOffset in
                                                      moveAudio(conversationId: conversationId, indices: indices, newOffset: newOffset)
                                                  })
                                }
                            }
                        }
                    }
                    .listStyle(InsetGroupedListStyle())
                    .onAppear {
                        DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
                            reader.scrollTo(audioManager.currentAudioID)
                        }
                    }
                }
                .navigationTitle("Downloaded Audios")
                .sheet(isPresented: Binding<Bool>(
                    get: { editingConversationId != nil },
                    set: { if !$0 { editingConversationId = nil } }
                )) {
                    if let conversationId = editingConversationId {
                        // Extracted view for editing conversation name
                        EditConversationView(conversationId: conversationId, newConversationName: $newConversationName, onCancel: {
                            editingConversationId = nil
                        }, onSave: {
                            saveNewConversationName(conversationId: conversationId, newName: newConversationName)
                            editingConversationId = nil
                        })
                    }
                }
                .alert(isPresented: $showErrorAlert) {
                    Alert(
                        title: Text("Error"),
                        message: Text(errorMessage),
                        dismissButton: .default(Text("OK"))
                    )
                }
            }
        }
    }

    private func deleteAudio(_ audio: DownloadedAudio) {
        guard webViewModel.webView.url?.host() == "chatgpt.com" else {
            errorMessage = "Failed to remove item: Host is not chatgpt.com"
            showErrorAlert = true
            return
        }

        Task {
            do {
                try await webViewModel.removeProcessedAudioItem(conversationId: audio.conversationId, messageId: audio.messageId)
                downloadedAudios.deleteAudio(audio)
            } catch {
                print("Remove processed audio item error: \(error.localizedDescription)")
                self.errorMessage = "Failed to remove item: \(error.localizedDescription)"
                self.showErrorAlert = true
            }
        }
    }

    private func startEditing(_ conversationId: UUID) {
        editingConversationId = conversationId
        newConversationName = downloadedAudios.getConversationName(by: conversationId)
    }

    private func saveNewConversationName(conversationId: UUID, newName: String) {
        downloadedAudios.updateConversationName(conversationId: conversationId, newName: newName)
    }

    private func moveAudio(conversationId: UUID, indices: IndexSet, newOffset: Int) {
        downloadedAudios.moveAudio(conversationId: conversationId, indices: indices, newOffset: newOffset)
    }
}

================
File: AudioFetchGPT/Views/DownloadedAudios/EditConversationView.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 6.10.24.
//
import SwiftUI

struct EditConversationView: View {
    var conversationId: UUID
    @Binding var newConversationName: String
    var onCancel: () -> Void
    var onSave: () -> Void

    var body: some View {
        VStack {
            Text("Edit Conversation Name")
                .font(.headline)
                .padding()

            TextField("New Conversation Name", text: $newConversationName)
                .textFieldStyle(RoundedBorderTextFieldStyle())
                .padding()

            HStack {
                Button("Cancel", action: onCancel)
                Spacer()
                Button("Save", action: onSave)
                    .disabled(newConversationName.isEmpty)
            }
            .padding()
        }
        .padding()
        .presentationDetents([.fraction(1 / 4)])
    }
}

================
File: AudioFetchGPT/Views/DownloadedAudios/SectionHeaderView.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 6.10.24.
//
import SwiftUI

struct SectionHeaderView: View {
    var conversationId: UUID
    var conversationName: String
    var onEdit: () -> Void
    var onToggle: () -> Void
    var isCollapsed: Bool // Added parameter to check if section is collapsed

    var body: some View {
        HStack {
            Text(conversationName)
                .lineLimit(3)
                .truncationMode(.middle)
                .font(.headline)
                .onTapGesture {
                    onEdit()
                }
            Spacer()
            Button(action: {
                onToggle()
            }) {
                Image(systemName: isCollapsed ? "chevron.right" : "chevron.down")
                    .foregroundColor(.blue)
            }
            .buttonStyle(BorderlessButtonStyle())
        }
    }
}

================
File: AudioFetchGPT/Views/MainContent/ControlButtonsView.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 20.09.24.
//
import SwiftUI

struct ControlButtonsView: View {
    @Binding var isSheetPresented: Bool
    var webViewModel: ConversationWebViewModel
    @Binding var isSearchVisible: Bool
    @Binding var searchText: String
    @State private var showMenu: Bool = false // State for showing/hiding menu
    @State private var showDownloadConfirmation: Bool = false 

    var body: some View {
        // Floating button
        VStack {
            Spacer()

            // Show all buttons with action descriptions if menu is open
            if showMenu {
                HStack {
                    Spacer()

                    VStack(alignment: .trailing, spacing: 20) {
                        // Navigation buttons (Previous/Next) in one line without text
                        HStack {
                            Button(action: {
                                webViewModel.scrollToPreviousReadAloudElement()
                            }) {
                                HStack {
                                    Image(systemName: "arrow.left.circle.fill")
                                        .resizable()
                                        .frame(width: 40, height: 40)
                                        .foregroundColor(.blue)
                                }
                                .padding()
                                .background(RoundedRectangle(cornerRadius: 10).fill(Color(.systemBackground)).shadow(radius: 5))
                            }

                            Button(action: {
                                webViewModel.scrollToNextReadAloudElement()
                            }) {
                                HStack {
                                    Image(systemName: "arrow.right.circle.fill")
                                        .resizable()
                                        .frame(width: 40, height: 40)
                                        .foregroundColor(.blue)
                                }
                                .padding()
                                .background(RoundedRectangle(cornerRadius: 10).fill(Color(.systemBackground)).shadow(radius: 5))
                            }
                        }

                        // Page management buttons (Reload/Search)
                        Button(action: {
                            webViewModel.reload()
                        }) {
                            HStack {
                                Image(systemName: "arrow.clockwise.circle.fill")
                                    .resizable()
                                    .frame(width: 40, height: 40)
                                    .foregroundColor(.green)
                                Text("Reload page")
                                    .foregroundColor(.primary)
                                    .font(.system(size: 16, weight: .bold))
                            }
                            .padding()
                            .background(RoundedRectangle(cornerRadius: 10).fill(Color(.systemBackground)).shadow(radius: 5))
                        }

                        Button(action: {
                            isSearchVisible.toggle()
                            if !isSearchVisible {
                                searchText = ""
                            }
                        }) {
                            HStack {
                                Image(systemName: isSearchVisible ? "magnifyingglass.circle.fill" : "magnifyingglass")
                                    .resizable()
                                    .frame(width: 40, height: 40)
                                    .foregroundColor(.orange)
                                Text(isSearchVisible ? "Hide search" : "Show search")
                                    .foregroundColor(.primary)
                                    .font(.system(size: 16, weight: .bold))
                            }
                            .padding()
                            .background(RoundedRectangle(cornerRadius: 10).fill(Color(.systemBackground)).shadow(radius: 5))
                        }

                        // Clipboard button
                        Button(action: {
                            if let clipboardText = UIPasteboard.general.string {
                                webViewModel.sayChatGPT(clipboardText)
                            }
                        }) {
                            HStack {
                                Image(systemName: "paperplane.circle.fill")
                                    .resizable()
                                    .frame(width: 40, height: 40)
                                    .foregroundColor(.purple)
                                Text("Send from Clipboard")
                                    .foregroundColor(.primary)
                                    .font(.system(size: 16, weight: .bold))
                            }
                            .padding()
                            .background(RoundedRectangle(cornerRadius: 10).fill(Color(.systemBackground)).shadow(radius: 5))
                        }

                        // Media-related buttons (Download/Show audios)
                        Button(action: {
                            showDownloadConfirmation = true
                        }) {
                            HStack {
                                Image(systemName: "arrow.down.circle.fill")
                                    .resizable()
                                    .frame(width: 40, height: 40)
                                    .foregroundColor(.yellow)
                                Text("Download all voices")
                                    .foregroundColor(.primary)
                                    .font(.system(size: 16, weight: .bold))
                            }
                            .padding()
                            .background(RoundedRectangle(cornerRadius: 10).fill(Color(.systemBackground)).shadow(radius: 5))
                        }
                        .alert(isPresented: $showDownloadConfirmation) {
                            Alert(
                                title: Text("Confirm Download"),
                                message: Text("Are you sure you want to download all voice messages? This may take some time."),
                                primaryButton: .default(Text("Yes"), action: {
                                    webViewModel.clickAllVoicePlayTurnActionButtons()
                                }),
                                secondaryButton: .cancel()
                            )
                        }


                        Button(action: {
                            isSheetPresented = true
                        }) {
                            HStack {
                                Image(systemName: "music.note")
                                    .resizable()
                                    .frame(width: 40, height: 40)
                                    .foregroundColor(.blue)
                                Text("Show downloaded audios")
                                    .foregroundColor(.primary)
                                    .font(.system(size: 16, weight: .bold))
                            }
                            .padding()
                            .background(RoundedRectangle(cornerRadius: 10).fill(Color(.systemBackground)).shadow(radius: 5))
                        }
                    }
                }
                .transition(.move(edge: .trailing)) // Adding animation
            }

            Spacer().frame(height: 70)

            // Floating button to show/hide menu
            Button(action: {
                withAnimation {
                    showMenu.toggle()
                }
            }) {
                Image(systemName: showMenu ? "xmark.circle.fill" : "plus.circle.fill")
                    .resizable()
                    .frame(width: 30, height: 30)
                    .foregroundColor(.blue)
            }
        }
        .padding(.bottom, 10) // Position the floating button at the bottom of the screen
        .padding(.trailing, 20)
    }
}

================
File: AudioFetchGPT/Views/MainContent/MainContentView.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 16.09.24.
//

import AVFoundation
import SwiftUI

struct MainContentView: View {
    @EnvironmentObject var downloadedAudios: DownloadedAudioStore
    @EnvironmentObject var audioManager: PlaybackManager
    @StateObject private var webViewModel = ConversationWebViewModel()
    @State private var isSheetPresented = false
    @State private var showNotification = false
    @State private var notificationMessage = ""
    @State private var searchText = ""
    @State private var searchForward = true
    @State private var isSearchVisible = false

    var body: some View {
        ZStack {
            ConversationWebView(viewModel: webViewModel)
                .environmentObject(downloadedAudios)
                .environmentObject(audioManager)

            if isSearchVisible {
                VStack {
                    SearchBarView(searchText: $searchText, searchForward: $searchForward) {
                        webViewModel.performSearch(text: searchText, forward: searchForward)
                    }
                    Spacer()
                }
            }

            ControlButtonsView(isSheetPresented: $isSheetPresented, webViewModel: webViewModel, isSearchVisible: $isSearchVisible, searchText: $searchText)

            if showNotification {
                NotificationBannerView(message: notificationMessage)
                    .onTapGesture {
                        showNotification = false
                    }
            }
        }
        .sheet(isPresented: $isSheetPresented) {
            DownloadedAudiosListView()
                .environmentObject(webViewModel)
        }
        .onAppear {
            downloadedAudios.loadDownloadedAudios()
            // Subscribe to notifications about download completion
            NotificationCenter.default.addObserver(forName: .audioDownloadCompleted, object: nil, queue: .main) { notification in
                if let audioName = notification.object as? String {
                    showDownloadNotification(for: audioName)
                }
            }
        }
        .onReceive(audioManager.$messageId) { _ in
            isSheetPresented = false
        }
    }

    // Function to show notification
    private func showDownloadNotification(for audioName: String) {
        notificationMessage = audioName
        showNotification = true
        DispatchQueue.main.asyncAfter(deadline: .now() + 10) {
            showNotification = false
        }
    }
}

#Preview {
    MainContentView()
}

================
File: AudioFetchGPT/Views/MainContent/NotificationBannerView.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 20.09.24.
//
import SwiftUI

// New view for notification
struct NotificationBannerView: View {
    var message: String

    var body: some View {
        VStack {
            Spacer()
            Text(message)
                .lineLimit(20)
                .padding()
                .background(Color.green)
                .foregroundColor(.white)
                .cornerRadius(10)
                .transition(.scale)
            Spacer()
        }
        .frame(maxWidth: .infinity, maxHeight: .infinity)
        .background(Color.black.opacity(0.5))
        .edgesIgnoringSafeArea(.all)
    }
}

================
File: AudioFetchGPT/Views/MainContent/SearchBarView.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 20.09.24.
//
import SwiftUI

// New view for search bar
struct SearchBarView: View {
    @Binding var searchText: String
    @Binding var searchForward: Bool
    var performSearch: () -> Void // New property
    @Environment(\.colorScheme) var colorScheme // Add this property

    var body: some View {
        HStack {
            ZStack(alignment: .trailing) {
                TextField("Search", text: $searchText)
                    .textFieldStyle(RoundedBorderTextFieldStyle())

                if !searchText.isEmpty {
                    Button(action: {
                        searchText = ""
                    }) {
                        Image(systemName: "xmark.circle.fill")
                            .foregroundColor(.gray)
                            .padding(.trailing, 8)
                    }
                }
            }

            Button(action: {
                searchForward = true
                performSearch()
            }) {
                Image(systemName: "arrow.down")
            }

            Button(action: {
                searchForward = false
                performSearch()
            }) {
                Image(systemName: "arrow.up")
            }
        }
        .padding()
        .background(colorScheme == .dark ? Color.black : Color.white) // Set background depending on theme
        .foregroundColor(colorScheme == .dark ? Color.white : Color.black) // Set text color depending on theme
        .shadow(radius: 5)
    }
}

================
File: AudioFetchGPT/AudioFetchGPTApp.swift
================
//
//  AudioFetchGPT
//
//  Created by Viktor Kushnerov on 16.09.24.
//

import SwiftUI

@main
struct AudioFetchGPTApp: App {
    @StateObject var downloadedAudios = DownloadedAudioStore()
    @StateObject var audioManager: PlaybackManager
    
    init() {
        let downloadedAudiosInstance = DownloadedAudioStore()
        _downloadedAudios = StateObject(wrappedValue: downloadedAudiosInstance)
        _audioManager = StateObject(wrappedValue: PlaybackManager(downloadedAudios: downloadedAudiosInstance))
    }
    
    var body: some Scene {
        WindowGroup {
            MainContentView()
                .environmentObject(downloadedAudios)
                .environmentObject(audioManager)
        }
    }
}

================
File: AudioFetchGPT/Info.plist
================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>UIBackgroundModes</key>
	<array>
		<string>audio</string>
	</array>
</dict>
</plist>

================
File: AudioFetchGPT.xcodeproj/project.xcworkspace/contents.xcworkspacedata
================
<?xml version="1.0" encoding="UTF-8"?>
<Workspace
   version = "1.0">
   <FileRef
      location = "self:">
   </FileRef>
</Workspace>

================
File: AudioFetchGPT.xcodeproj/xcshareddata/xcschemes/AudioFetchGPT_Debugger.xcscheme
================
<?xml version="1.0" encoding="UTF-8"?>
<Scheme
   LastUpgradeVersion = "1600"
   version = "1.7">
   <BuildAction
      parallelizeBuildables = "YES"
      buildImplicitDependencies = "YES"
      buildArchitectures = "Automatic">
      <BuildActionEntries>
         <BuildActionEntry
            buildForTesting = "YES"
            buildForRunning = "YES"
            buildForProfiling = "YES"
            buildForArchiving = "YES"
            buildForAnalyzing = "YES">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "F071E9CE2C98303900423E5F"
               BuildableName = "AudioFetchGPT.app"
               BlueprintName = "AudioFetchGPT"
               ReferencedContainer = "container:AudioFetchGPT.xcodeproj">
            </BuildableReference>
         </BuildActionEntry>
      </BuildActionEntries>
   </BuildAction>
   <TestAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      shouldUseLaunchSchemeArgsEnv = "YES"
      shouldAutocreateTestPlan = "YES">
      <Testables>
         <TestableReference
            skipped = "NO"
            parallelizable = "YES">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "F02CB09B2CA097E7006F5441"
               BuildableName = "AudioFetchGPTTests.xctest"
               BlueprintName = "AudioFetchGPTTests"
               ReferencedContainer = "container:AudioFetchGPT.xcodeproj">
            </BuildableReference>
         </TestableReference>
      </Testables>
   </TestAction>
   <LaunchAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      launchStyle = "0"
      useCustomWorkingDirectory = "NO"
      ignoresPersistentStateOnLaunch = "NO"
      debugDocumentVersioning = "YES"
      debugServiceExtension = "internal"
      allowLocationSimulation = "YES">
      <BuildableProductRunnable
         runnableDebuggingMode = "0">
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "F071E9CE2C98303900423E5F"
            BuildableName = "AudioFetchGPT.app"
            BlueprintName = "AudioFetchGPT"
            ReferencedContainer = "container:AudioFetchGPT.xcodeproj">
         </BuildableReference>
      </BuildableProductRunnable>
   </LaunchAction>
   <ProfileAction
      buildConfiguration = "Release"
      shouldUseLaunchSchemeArgsEnv = "YES"
      savedToolIdentifier = ""
      useCustomWorkingDirectory = "NO"
      debugDocumentVersioning = "YES">
      <BuildableProductRunnable
         runnableDebuggingMode = "0">
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "F071E9CE2C98303900423E5F"
            BuildableName = "AudioFetchGPT.app"
            BlueprintName = "AudioFetchGPT"
            ReferencedContainer = "container:AudioFetchGPT.xcodeproj">
         </BuildableReference>
      </BuildableProductRunnable>
   </ProfileAction>
   <AnalyzeAction
      buildConfiguration = "Debug">
   </AnalyzeAction>
   <ArchiveAction
      buildConfiguration = "Release"
      revealArchiveInOrganizer = "YES">
   </ArchiveAction>
</Scheme>

================
File: AudioFetchGPT.xcodeproj/xcshareddata/xcschemes/AudioFetchGPT.xcscheme
================
<?xml version="1.0" encoding="UTF-8"?>
<Scheme
   LastUpgradeVersion = "1600"
   version = "1.7">
   <BuildAction
      parallelizeBuildables = "YES"
      buildImplicitDependencies = "YES"
      buildArchitectures = "Automatic">
      <BuildActionEntries>
         <BuildActionEntry
            buildForTesting = "YES"
            buildForRunning = "YES"
            buildForProfiling = "YES"
            buildForArchiving = "YES"
            buildForAnalyzing = "YES">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "F071E9CE2C98303900423E5F"
               BuildableName = "AudioFetchGPT.app"
               BlueprintName = "AudioFetchGPT"
               ReferencedContainer = "container:AudioFetchGPT.xcodeproj">
            </BuildableReference>
         </BuildActionEntry>
      </BuildActionEntries>
   </BuildAction>
   <TestAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      shouldUseLaunchSchemeArgsEnv = "YES"
      shouldAutocreateTestPlan = "YES">
      <Testables>
         <TestableReference
            skipped = "NO"
            parallelizable = "YES">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "F02CB09B2CA097E7006F5441"
               BuildableName = "AudioFetchGPTTests.xctest"
               BlueprintName = "AudioFetchGPTTests"
               ReferencedContainer = "container:AudioFetchGPT.xcodeproj">
            </BuildableReference>
         </TestableReference>
      </Testables>
   </TestAction>
   <LaunchAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = ""
      selectedLauncherIdentifier = "Xcode.IDEFoundation.Launcher.PosixSpawn"
      launchStyle = "0"
      useCustomWorkingDirectory = "NO"
      ignoresPersistentStateOnLaunch = "NO"
      debugDocumentVersioning = "YES"
      debugServiceExtension = "internal"
      allowLocationSimulation = "YES">
      <BuildableProductRunnable
         runnableDebuggingMode = "0">
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "F071E9CE2C98303900423E5F"
            BuildableName = "AudioFetchGPT.app"
            BlueprintName = "AudioFetchGPT"
            ReferencedContainer = "container:AudioFetchGPT.xcodeproj">
         </BuildableReference>
      </BuildableProductRunnable>
   </LaunchAction>
   <ProfileAction
      buildConfiguration = "Release"
      shouldUseLaunchSchemeArgsEnv = "YES"
      savedToolIdentifier = ""
      useCustomWorkingDirectory = "NO"
      debugDocumentVersioning = "YES">
      <BuildableProductRunnable
         runnableDebuggingMode = "0">
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "F071E9CE2C98303900423E5F"
            BuildableName = "AudioFetchGPT.app"
            BlueprintName = "AudioFetchGPT"
            ReferencedContainer = "container:AudioFetchGPT.xcodeproj">
         </BuildableReference>
      </BuildableProductRunnable>
   </ProfileAction>
   <AnalyzeAction
      buildConfiguration = "Debug">
   </AnalyzeAction>
   <ArchiveAction
      buildConfiguration = "Release"
      revealArchiveInOrganizer = "YES">
   </ArchiveAction>
</Scheme>

================
File: AudioFetchGPT.xcodeproj/xcshareddata/xcschemes/AudioFetchGPTTests.xcscheme
================
<?xml version="1.0" encoding="UTF-8"?>
<Scheme
   LastUpgradeVersion = "1600"
   version = "1.7">
   <BuildAction
      parallelizeBuildables = "YES"
      buildImplicitDependencies = "YES"
      buildArchitectures = "Automatic">
   </BuildAction>
   <TestAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      shouldUseLaunchSchemeArgsEnv = "YES"
      shouldAutocreateTestPlan = "YES">
      <Testables>
         <TestableReference
            skipped = "NO"
            parallelizable = "YES">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "F02CB09B2CA097E7006F5441"
               BuildableName = "AudioFetchGPTTests.xctest"
               BlueprintName = "AudioFetchGPTTests"
               ReferencedContainer = "container:AudioFetchGPT.xcodeproj">
            </BuildableReference>
         </TestableReference>
      </Testables>
   </TestAction>
   <LaunchAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      launchStyle = "0"
      useCustomWorkingDirectory = "NO"
      ignoresPersistentStateOnLaunch = "NO"
      debugDocumentVersioning = "YES"
      debugServiceExtension = "internal"
      allowLocationSimulation = "YES">
      <BuildableProductRunnable
         runnableDebuggingMode = "0">
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "F071E9CE2C98303900423E5F"
            BuildableName = "AudioFetchGPT.app"
            BlueprintName = "AudioFetchGPT"
            ReferencedContainer = "container:AudioFetchGPT.xcodeproj">
         </BuildableReference>
      </BuildableProductRunnable>
   </LaunchAction>
   <ProfileAction
      buildConfiguration = "Release"
      shouldUseLaunchSchemeArgsEnv = "YES"
      savedToolIdentifier = ""
      useCustomWorkingDirectory = "NO"
      debugDocumentVersioning = "YES">
      <MacroExpansion>
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "F071E9CE2C98303900423E5F"
            BuildableName = "AudioFetchGPT.app"
            BlueprintName = "AudioFetchGPT"
            ReferencedContainer = "container:AudioFetchGPT.xcodeproj">
         </BuildableReference>
      </MacroExpansion>
   </ProfileAction>
   <AnalyzeAction
      buildConfiguration = "Debug">
   </AnalyzeAction>
   <ArchiveAction
      buildConfiguration = "Release"
      revealArchiveInOrganizer = "YES">
   </ArchiveAction>
</Scheme>

================
File: AudioFetchGPT.xcodeproj/project.pbxproj
================
// !$*UTF8*$!
{
	archiveVersion = 1;
	classes = {
	};
	objectVersion = 77;
	objects = {

/* Begin PBXContainerItemProxy section */
		F02CB0A02CA097E7006F5441 /* PBXContainerItemProxy */ = {
			isa = PBXContainerItemProxy;
			containerPortal = F071E9C72C98303900423E5F /* Project object */;
			proxyType = 1;
			remoteGlobalIDString = F071E9CE2C98303900423E5F;
			remoteInfo = AudioFetchGPT;
		};
/* End PBXContainerItemProxy section */

/* Begin PBXFileReference section */
		F02CB09C2CA097E7006F5441 /* AudioFetchGPTTests.xctest */ = {isa = PBXFileReference; explicitFileType = wrapper.cfbundle; includeInIndex = 0; path = AudioFetchGPTTests.xctest; sourceTree = BUILT_PRODUCTS_DIR; };
		F071E9CF2C98303900423E5F /* AudioFetchGPT.app */ = {isa = PBXFileReference; explicitFileType = wrapper.application; includeInIndex = 0; path = AudioFetchGPT.app; sourceTree = BUILT_PRODUCTS_DIR; };
		F071E9EE2C98A47E00423E5F /* .gitignore */ = {isa = PBXFileReference; lastKnownFileType = text; path = .gitignore; sourceTree = "<group>"; };
		F0C5E6FA2CAE5F6B009F0441 /* README.md */ = {isa = PBXFileReference; lastKnownFileType = net.daringfireball.markdown; path = README.md; sourceTree = "<group>"; };
/* End PBXFileReference section */

/* Begin PBXFileSystemSynchronizedBuildFileExceptionSet section */
		F071E9E92C98428C00423E5F /* Exceptions for "AudioFetchGPT" folder in "AudioFetchGPT" target */ = {
			isa = PBXFileSystemSynchronizedBuildFileExceptionSet;
			membershipExceptions = (
				Info.plist,
			);
			target = F071E9CE2C98303900423E5F /* AudioFetchGPT */;
		};
		F0C5E6FB2CAE5F74009F0441 /* Exceptions for "Assets" folder in "AudioFetchGPT" target */ = {
			isa = PBXFileSystemSynchronizedBuildFileExceptionSet;
			membershipExceptions = (
				"downloaded-view.jpeg",
				"texteditor-view.jpeg",
			);
			target = F071E9CE2C98303900423E5F /* AudioFetchGPT */;
		};
/* End PBXFileSystemSynchronizedBuildFileExceptionSet section */

/* Begin PBXFileSystemSynchronizedRootGroup section */
		F02CB09D2CA097E7006F5441 /* AudioFetchGPTTests */ = {
			isa = PBXFileSystemSynchronizedRootGroup;
			path = AudioFetchGPTTests;
			sourceTree = "<group>";
		};
		F071E9D12C98303900423E5F /* AudioFetchGPT */ = {
			isa = PBXFileSystemSynchronizedRootGroup;
			exceptions = (
				F071E9E92C98428C00423E5F /* Exceptions for "AudioFetchGPT" folder in "AudioFetchGPT" target */,
			);
			path = AudioFetchGPT;
			sourceTree = "<group>";
		};
		F0C5E6ED2CAE5EB0009F0441 /* Assets */ = {
			isa = PBXFileSystemSynchronizedRootGroup;
			exceptions = (
				F0C5E6FB2CAE5F74009F0441 /* Exceptions for "Assets" folder in "AudioFetchGPT" target */,
			);
			path = Assets;
			sourceTree = "<group>";
		};
/* End PBXFileSystemSynchronizedRootGroup section */

/* Begin PBXFrameworksBuildPhase section */
		F02CB0992CA097E7006F5441 /* Frameworks */ = {
			isa = PBXFrameworksBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
		F071E9CC2C98303900423E5F /* Frameworks */ = {
			isa = PBXFrameworksBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXFrameworksBuildPhase section */

/* Begin PBXGroup section */
		F071E9C62C98303900423E5F = {
			isa = PBXGroup;
			children = (
				F0C5E6FA2CAE5F6B009F0441 /* README.md */,
				F0C5E6ED2CAE5EB0009F0441 /* Assets */,
				F071E9EE2C98A47E00423E5F /* .gitignore */,
				F071E9D12C98303900423E5F /* AudioFetchGPT */,
				F02CB09D2CA097E7006F5441 /* AudioFetchGPTTests */,
				F071E9D02C98303900423E5F /* Products */,
			);
			sourceTree = "<group>";
		};
		F071E9D02C98303900423E5F /* Products */ = {
			isa = PBXGroup;
			children = (
				F071E9CF2C98303900423E5F /* AudioFetchGPT.app */,
				F02CB09C2CA097E7006F5441 /* AudioFetchGPTTests.xctest */,
			);
			name = Products;
			sourceTree = "<group>";
		};
/* End PBXGroup section */

/* Begin PBXNativeTarget section */
		F02CB09B2CA097E7006F5441 /* AudioFetchGPTTests */ = {
			isa = PBXNativeTarget;
			buildConfigurationList = F02CB0A22CA097E7006F5441 /* Build configuration list for PBXNativeTarget "AudioFetchGPTTests" */;
			buildPhases = (
				F02CB0982CA097E7006F5441 /* Sources */,
				F02CB0992CA097E7006F5441 /* Frameworks */,
				F02CB09A2CA097E7006F5441 /* Resources */,
			);
			buildRules = (
			);
			dependencies = (
				F02CB0A12CA097E7006F5441 /* PBXTargetDependency */,
			);
			fileSystemSynchronizedGroups = (
				F02CB09D2CA097E7006F5441 /* AudioFetchGPTTests */,
			);
			name = AudioFetchGPTTests;
			packageProductDependencies = (
			);
			productName = AudioFetchGPTTests;
			productReference = F02CB09C2CA097E7006F5441 /* AudioFetchGPTTests.xctest */;
			productType = "com.apple.product-type.bundle.unit-test";
		};
		F071E9CE2C98303900423E5F /* AudioFetchGPT */ = {
			isa = PBXNativeTarget;
			buildConfigurationList = F071E9DD2C98303A00423E5F /* Build configuration list for PBXNativeTarget "AudioFetchGPT" */;
			buildPhases = (
				F071E9CB2C98303900423E5F /* Sources */,
				F071E9CC2C98303900423E5F /* Frameworks */,
				F071E9CD2C98303900423E5F /* Resources */,
			);
			buildRules = (
			);
			dependencies = (
			);
			fileSystemSynchronizedGroups = (
				F071E9D12C98303900423E5F /* AudioFetchGPT */,
				F0C5E6ED2CAE5EB0009F0441 /* Assets */,
			);
			name = AudioFetchGPT;
			packageProductDependencies = (
			);
			productName = AudioFetchGPT;
			productReference = F071E9CF2C98303900423E5F /* AudioFetchGPT.app */;
			productType = "com.apple.product-type.application";
		};
/* End PBXNativeTarget section */

/* Begin PBXProject section */
		F071E9C72C98303900423E5F /* Project object */ = {
			isa = PBXProject;
			attributes = {
				BuildIndependentTargetsInParallel = 1;
				LastSwiftUpdateCheck = 1600;
				LastUpgradeCheck = 1600;
				TargetAttributes = {
					F02CB09B2CA097E7006F5441 = {
						CreatedOnToolsVersion = 16.0;
						TestTargetID = F071E9CE2C98303900423E5F;
					};
					F071E9CE2C98303900423E5F = {
						CreatedOnToolsVersion = 16.0;
					};
				};
			};
			buildConfigurationList = F071E9CA2C98303900423E5F /* Build configuration list for PBXProject "AudioFetchGPT" */;
			developmentRegion = en;
			hasScannedForEncodings = 0;
			knownRegions = (
				en,
				Base,
			);
			mainGroup = F071E9C62C98303900423E5F;
			minimizedProjectReferenceProxies = 1;
			preferredProjectObjectVersion = 77;
			productRefGroup = F071E9D02C98303900423E5F /* Products */;
			projectDirPath = "";
			projectRoot = "";
			targets = (
				F071E9CE2C98303900423E5F /* AudioFetchGPT */,
				F02CB09B2CA097E7006F5441 /* AudioFetchGPTTests */,
			);
		};
/* End PBXProject section */

/* Begin PBXResourcesBuildPhase section */
		F02CB09A2CA097E7006F5441 /* Resources */ = {
			isa = PBXResourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
		F071E9CD2C98303900423E5F /* Resources */ = {
			isa = PBXResourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXResourcesBuildPhase section */

/* Begin PBXSourcesBuildPhase section */
		F02CB0982CA097E7006F5441 /* Sources */ = {
			isa = PBXSourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
		F071E9CB2C98303900423E5F /* Sources */ = {
			isa = PBXSourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXSourcesBuildPhase section */

/* Begin PBXTargetDependency section */
		F02CB0A12CA097E7006F5441 /* PBXTargetDependency */ = {
			isa = PBXTargetDependency;
			target = F071E9CE2C98303900423E5F /* AudioFetchGPT */;
			targetProxy = F02CB0A02CA097E7006F5441 /* PBXContainerItemProxy */;
		};
/* End PBXTargetDependency section */

/* Begin XCBuildConfiguration section */
		F02CB0A32CA097E7006F5441 /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				BUNDLE_LOADER = "$(TEST_HOST)";
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_TEAM = B6QRXW5YF2;
				GENERATE_INFOPLIST_FILE = YES;
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = by.filimo.AudioFetchGPTTests;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SWIFT_EMIT_LOC_STRINGS = NO;
				SWIFT_VERSION = 5.0;
				TARGETED_DEVICE_FAMILY = "1,2";
				TEST_HOST = "$(BUILT_PRODUCTS_DIR)/AudioFetchGPT.app/$(BUNDLE_EXECUTABLE_FOLDER_PATH)/AudioFetchGPT";
			};
			name = Debug;
		};
		F02CB0A42CA097E7006F5441 /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				BUNDLE_LOADER = "$(TEST_HOST)";
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_TEAM = B6QRXW5YF2;
				GENERATE_INFOPLIST_FILE = YES;
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = by.filimo.AudioFetchGPTTests;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SWIFT_EMIT_LOC_STRINGS = NO;
				SWIFT_VERSION = 5.0;
				TARGETED_DEVICE_FAMILY = "1,2";
				TEST_HOST = "$(BUILT_PRODUCTS_DIR)/AudioFetchGPT.app/$(BUNDLE_EXECUTABLE_FOLDER_PATH)/AudioFetchGPT";
			};
			name = Release;
		};
		F071E9DB2C98303A00423E5F /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = dwarf;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				ENABLE_TESTABILITY = YES;
				ENABLE_USER_SCRIPT_SANDBOXING = YES;
				GCC_C_LANGUAGE_STANDARD = gnu17;
				GCC_DYNAMIC_NO_PIC = NO;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_OPTIMIZATION_LEVEL = 0;
				GCC_PREPROCESSOR_DEFINITIONS = (
					"DEBUG=1",
					"$(inherited)",
				);
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 18.0;
				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
				MTL_FAST_MATH = YES;
				ONLY_ACTIVE_ARCH = YES;
				SDKROOT = iphoneos;
				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
			};
			name = Debug;
		};
		F071E9DC2C98303A00423E5F /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
				ENABLE_NS_ASSERTIONS = NO;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				ENABLE_USER_SCRIPT_SANDBOXING = YES;
				GCC_C_LANGUAGE_STANDARD = gnu17;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 18.0;
				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
				MTL_ENABLE_DEBUG_INFO = NO;
				MTL_FAST_MATH = YES;
				SDKROOT = iphoneos;
				SWIFT_COMPILATION_MODE = wholemodule;
				VALIDATE_PRODUCT = YES;
			};
			name = Release;
		};
		F071E9DE2C98303A00423E5F /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_ASSET_PATHS = "\"AudioFetchGPT/Preview Content\"";
				DEVELOPMENT_TEAM = B6QRXW5YF2;
				ENABLE_PREVIEWS = YES;
				GENERATE_INFOPLIST_FILE = YES;
				INFOPLIST_FILE = AudioFetchGPT/Info.plist;
				INFOPLIST_KEY_UIApplicationSceneManifest_Generation = YES;
				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				LD_RUNPATH_SEARCH_PATHS = (
					"$(inherited)",
					"@executable_path/Frameworks",
				);
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = by.filimo.AudioFetchGPT;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SWIFT_EMIT_LOC_STRINGS = YES;
				SWIFT_VERSION = 5.0;
				TARGETED_DEVICE_FAMILY = "1,2";
			};
			name = Debug;
		};
		F071E9DF2C98303A00423E5F /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_ASSET_PATHS = "\"AudioFetchGPT/Preview Content\"";
				DEVELOPMENT_TEAM = B6QRXW5YF2;
				ENABLE_PREVIEWS = YES;
				GENERATE_INFOPLIST_FILE = YES;
				INFOPLIST_FILE = AudioFetchGPT/Info.plist;
				INFOPLIST_KEY_UIApplicationSceneManifest_Generation = YES;
				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				LD_RUNPATH_SEARCH_PATHS = (
					"$(inherited)",
					"@executable_path/Frameworks",
				);
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = by.filimo.AudioFetchGPT;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SWIFT_EMIT_LOC_STRINGS = YES;
				SWIFT_VERSION = 5.0;
				TARGETED_DEVICE_FAMILY = "1,2";
			};
			name = Release;
		};
/* End XCBuildConfiguration section */

/* Begin XCConfigurationList section */
		F02CB0A22CA097E7006F5441 /* Build configuration list for PBXNativeTarget "AudioFetchGPTTests" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				F02CB0A32CA097E7006F5441 /* Debug */,
				F02CB0A42CA097E7006F5441 /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
		F071E9CA2C98303900423E5F /* Build configuration list for PBXProject "AudioFetchGPT" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				F071E9DB2C98303A00423E5F /* Debug */,
				F071E9DC2C98303A00423E5F /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
		F071E9DD2C98303A00423E5F /* Build configuration list for PBXNativeTarget "AudioFetchGPT" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				F071E9DE2C98303A00423E5F /* Debug */,
				F071E9DF2C98303A00423E5F /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
/* End XCConfigurationList section */
	};
	rootObject = F071E9C72C98303900423E5F /* Project object */;
}

================
File: AudioFetchGPTTests/AudioFetchGPTTests.swift
================
//
//  AudioFetchGPTTests
//
//  Created by Viktor Kushnerov on 22.09.24.
//

import Testing

struct AudioFetchGPTTests {

    @Test func example() async throws {
        // Write your test here and use APIs like `#expect(...)` to check expected conditions.
    }

}

================
File: .cursorrules
================
You are an expert AI programming assistant that primarily focuses on producing clear, readable SwiftUI code.

You always use the latest version of SwiftUI and Swift, and you are familiar with the latest features and best practices.

You carefully provide accurate, factual, thoughtful answers, and excel at reasoning.

- Follow the user's requirements carefully & to the letter.
- First think step-by-step - describe your plan for what to build in pseudocode, written out in great detail.
- Confirm, then write code!
- Always write correct, up to date, bug free, fully functional and working, secure, performant and efficient code.
- Focus on readability over being performant.
- Fully implement all requested functionality.
- Leave NO todo's, placeholders or missing pieces.
- Be concise. Minimize any other prose.
- If you think there might not be a correct answer, you say so. If you do not know the answer, say so instead of guessing.


- Comments in the code must be in Russian.
- When implementing multithreading or asynchronous code, always include comments explaining why concurrency is used in that specific case. This improves readability and helps other developers understand the context.
- Your answers must be in Russian.

================
File: .gitignore
================
# Created by https://www.toptal.com/developers/gitignore/api/xcode
# Edit at https://www.toptal.com/developers/gitignore?templates=xcode

### Xcode ###
## User settings
xcuserdata/

## Xcode 8 and earlier
*.xcscmblueprint
*.xccheckout

### Xcode Patch ###
*.xcodeproj/*
!*.xcodeproj/project.pbxproj
!*.xcodeproj/xcshareddata/
!*.xcodeproj/project.xcworkspace/
!*.xcworkspace/contents.xcworkspacedata
/*.gcno
**/xcshareddata/WorkspaceSettings.xcsettings

# End of https://www.toptal.com/developers/gitignore/api/xcode


.DS_Store

================
File: README.md
================
# AudioFetchGPT Documentation

## Overview

AudioFetchGPT is an iOS application designed to interact with ChatGPT, allowing users to download and manage audio content from conversations. The app provides a web interface to ChatGPT, audio playback capabilities, and a download management system.


<p float="left">
  <img src="./Assets/main-view.jpeg" width="200" />
  <img src="./Assets/downloaded-view.jpeg" width="200" />
  <img src="./Assets/texteditor-view.jpeg" width="200" />
  <img src="./Assets/lockscreen.jpg" width="200" />
</p>


## Key Features

1. Web Interface: Integrates a WebView to interact with ChatGPT.
2. Audio Download: Captures and downloads audio content from ChatGPT conversations.
3. Audio Playback: Allows users to play, pause, and seek through downloaded audio files.
4. Download Management: Provides a list view of downloaded audio files with editing and deletion capabilities.
5. Search Functionality: Enables users to search within the ChatGPT interface.
6. Background Audio: Supports background audio playback and control through the iOS audio session.

## Key Functionalities

1. **Audio Download**: The app injects a JavaScript script into the WebView to intercept audio synthesis requests. When an audio file is generated, it's captured and downloaded to the device.

2. **Audio Playback**: Users can play, pause, and seek through downloaded audio files. The app supports background audio and integrates with the iOS audio session for system-wide audio controls.

3. **Download Management**: Users can view, rename, and delete downloaded audio files. The app maintains a list of downloads and their metadata.

4. **Web Interaction**: The app provides a custom interface for interacting with ChatGPT, including search functionality and the ability to navigate to specific messages.


## Input and Output

### Inputs
- User interactions with the ChatGPT interface
- Audio playback controls (play, pause, seek)
- Download management actions (rename, delete)
- Search queries

### Outputs
- Rendered WebView of ChatGPT
- List of downloaded audio files
- Audio playback
- Visual feedback for user actions (notifications, UI updates)

## Usage

The app is designed to be used as a ChatGPT client with enhanced audio capabilities. Users can interact with ChatGPT as normal, with the added ability to download and manage audio content generated during conversations. The downloaded audio can be played back at any time, even without an internet connection.
